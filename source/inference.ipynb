{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "836ILJIsc-oS",
        "outputId": "6224aabd-2ecd-4194-bc2d-1c3f87294bc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "IEEmUCpve2Q-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")\n",
        "\n",
        "gen_peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")"
      ],
      "metadata": {
        "id": "gsbgkrGBd5SR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분류 모델"
      ],
      "metadata": {
        "id": "mvq39FctdbsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "cls_path = '/models/kogpt2-classification-lora'\n",
        "cls_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      cls_path,\n",
        "      num_labels=5,\n",
        "      problem_type=\"multi_label_classification\"\n",
        ")\n",
        "trained_cls_model = get_peft_model(cls_model, cls_peft_config)\n",
        "trained_cls_tokenizer = PreTrainedTokenizerFast.from_pretrained(cls_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3QIcNtKdeG8",
        "outputId": "1da47cd6-a38f-49d0-c716-42232a736a90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_listener_empathy(input_text, model, tokenizer, num_classes=5, threshold=0.6):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 문장 토큰화\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # 모델에 입력을 전달하여 로짓(logits)을 얻음\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 로짓에 시그모이드 적용하여 확률로 변환\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "    # 임계값을 기준으로 이진화\n",
        "    predictions = (probabilities > threshold).int()\n",
        "\n",
        "    # 레이블 디코딩\n",
        "    label_classes = [0, 1, 2, 3, 4]\n",
        "    predicted_labels = [label_classes[i] for i in range(num_classes) if predictions[0][i] == 1]\n",
        "\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "37UV33FfdkIu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 생성 모델"
      ],
      "metadata": {
        "id": "94P400-5dXut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "gen_path = '/models/kogpt2-chatbot'\n",
        "gen_model = GPT2LMHeadModel.from_pretrained(gen_path)\n",
        "\n",
        "trained_gen_model = get_peft_model(gen_model, gen_peft_config)\n",
        "trained_gen_tokenizer = PreTrainedTokenizerFast.from_pretrained(gen_path)"
      ],
      "metadata": {
        "id": "A4JdPr3bdDJ-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(predicted_labels, input_text, model, tokenizer):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "    # 입력 문장 토큰화\n",
        "    condition = ' ,'.join(map(str, predicted_labels))\n",
        "    inputs = f\"{condition}를 중심으로 \\\"{input_text}\\\"에 대해 대답해줘\"\n",
        "    input_ids = tokenizer.encode(tokenizer.bos_token + inputs + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # 모델 추론\n",
        "    outputs = model.generate(input_ids, max_length=50, repetition_penalty=2.0, num_beams=5, early_stopping=True)\n",
        "    output_text = trained_gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "jIw9-2aDdRDH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론"
      ],
      "metadata": {
        "id": "4m1mAQhgg2U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 입력 문장\n",
        "input_text = \"오늘 기분은 어때?\"\n",
        "\n",
        "# 분류 결과 추론\n",
        "# threshold 잘 설정해야\n",
        "predicted_labels = predict_listener_empathy(input_text, trained_cls_model, trained_cls_tokenizer, threshold=0.5)\n",
        "print(f\"Predicted labels: {predicted_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KawhJ1dnER",
        "outputId": "2515df59-4ff6-47af-e366-15dee44a21a9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [0, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_answer(predicted_labels, input_text, trained_gen_model, trained_gen_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfYiXHggMgG",
        "outputId": "5e298633-bb87-44cb-aa47-f461fe2e62b7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0,4를 중심으로 \"오늘 기분은 어때?\"에 대해 대답해줘\n"
          ]
        }
      ]
    }
  ]
}