{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "836ILJIsc-oS",
        "outputId": "363d3103-bc91-420b-fc9d-9455e13d9eb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "IEEmUCpve2Q-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수\n",
        "# 그 외 추가 가능 인자 https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "\n",
        "threshold=0.6 # float, 0~1\n",
        "max_length = 50 # int, default=20\n",
        "min_new_tokens = 2 # int\n",
        "use_cache = True # bool, default=True\n",
        "repetition_penalty = 5.0 # float, default=1.0\n",
        "do_sample = True # bool, default=False\n",
        "num_beams = 5 # int, default=1\n",
        "temperature = 2.0 # float, default=1.0\n",
        "top_k = 50 # int, default=50\n",
        "top_p = 0.9 # float, default=1.0"
      ],
      "metadata": {
        "id": "D4tVgEfknhuT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    inference_mode=True,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")\n",
        "\n",
        "gen_peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=True,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")"
      ],
      "metadata": {
        "id": "gsbgkrGBd5SR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_TKN = \"<Q>\"\n",
        "A_TKN = \"<A>\"\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "UNK = '<unk>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'"
      ],
      "metadata": {
        "id": "5f5SKs1Gi0n2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분류 모델"
      ],
      "metadata": {
        "id": "mvq39FctdbsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "cls_path = '/content/drive/MyDrive/Colab Notebooks/kogpt2-classification'\n",
        "cls_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      cls_path,\n",
        "      num_labels=5,\n",
        "      problem_type=\"multi_label_classification\"\n",
        ")\n",
        "trained_cls_model = get_peft_model(cls_model, cls_peft_config)\n",
        "trained_cls_tokenizer = PreTrainedTokenizerFast.from_pretrained(cls_path,\n",
        "                          bos_token=BOS, eos_token=EOS, unk_token=UNK,\n",
        "                          pad_token=PAD, mask_token=MASK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3QIcNtKdeG8",
        "outputId": "d5d729b9-12e7-412b-c107-e089e16b7288"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_listener_empathy(input_text, model, tokenizer, threshold=threshold):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 문장 토큰화\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # 모델에 입력을 전달하여 로짓(logits)을 얻음\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 로짓에 시그모이드 적용하여 확률로 변환\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "    # 임계값을 기준으로 이진화\n",
        "    predictions = (probabilities > threshold).int()\n",
        "\n",
        "    # 레이블 디코딩\n",
        "    label_classes = ['조언', '격려', '위로', '동조', '']\n",
        "    num_classes = 5\n",
        "    predicted_labels = [label_classes[i] for i in range(num_classes) if predictions[0][i] == 1]\n",
        "\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "37UV33FfdkIu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 생성 모델"
      ],
      "metadata": {
        "id": "94P400-5dXut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "gen_path = '/content/drive/MyDrive/Colab Notebooks/kogpt2-chatbot'\n",
        "gen_model = GPT2LMHeadModel.from_pretrained(gen_path)\n",
        "\n",
        "trained_gen_model = get_peft_model(gen_model, gen_peft_config)\n",
        "trained_gen_tokenizer = PreTrainedTokenizerFast.from_pretrained(gen_path,\n",
        "                          bos_token=BOS, eos_token=EOS, unk_token=UNK,\n",
        "                          pad_token=PAD, mask_token=MASK)"
      ],
      "metadata": {
        "id": "A4JdPr3bdDJ-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(predicted_labels, input_text, model, tokenizer,\n",
        "                   max_length, min_new_tokens, use_cache,\n",
        "                   repetition_penalty, do_sample, num_beams,\n",
        "                   temperature, top_k, top_p):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "    # 입력 문장 토큰화\n",
        "    empathy = ' ,'.join(map(str, predicted_labels))\n",
        "    inputs = Q_TKN + input_text + SENT + empathy + A_TKN\n",
        "    input_ids = tokenizer.encode(tokenizer.bos_token + inputs + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # 모델 추론\n",
        "    outputs = model.generate(input_ids,\n",
        "                             max_length=max_length, min_new_tokens=min_new_tokens, use_cache=use_cache,\n",
        "                             repetition_penalty=repetition_penalty, do_sample=do_sample, num_beams=num_beams,\n",
        "                             temperature=temperature, top_k=top_k, top_p=top_p, early_stopping=True)\n",
        "    output_text = trained_gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text = output_text.split(A_TKN)[1]\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "jIw9-2aDdRDH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론"
      ],
      "metadata": {
        "id": "4m1mAQhgg2U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 입력 문장\n",
        "input_text = \"오늘 기분은 어때?...\"\n",
        "\n",
        "# 분류 결과 추론\n",
        "# threshold 잘 설정해야\n",
        "predicted_labels = predict_listener_empathy(input_text, trained_cls_model, trained_cls_tokenizer, threshold)\n",
        "print(f\"Predicted labels: {predicted_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KawhJ1dnER",
        "outputId": "c7ebbb05-9460-40a7-bab5-cc90e70e7fdc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: ['조언']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = predict_answer(predicted_labels, input_text, trained_gen_model, trained_gen_tokenizer,\n",
        "                   max_length, min_new_tokens, use_cache,\n",
        "                   repetition_penalty, do_sample, num_beams,\n",
        "                   temperature, top_k, top_p)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfYiXHggMgG",
        "outputId": "3696b06b-eb24-44de-fbe4-85c6c6c228af"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 통찰력을 가진 자였으니까..\n",
            "(중략 )\n",
            "#책스타그램 #북스타그램 #독서스타그램 #독서 #\n"
          ]
        }
      ]
    }
  ]
}